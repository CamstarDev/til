---
#自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出
#定向调度：NodeName、NodeSelector
#亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity
#污点（容忍）调度：Taints、Toleration
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    name: myapp
spec:
  containers:
    - name: myapp
      image: <Image>
      resources:
        limits:
          memory: "128Mi"
          cpu: "500m"
      ports:
        - containerPort: 8083
 
 
  #定向调度 使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行
  nodeName: node1
  nodeSelector:
    version: 1.0.01
    devtype: dev
  #亲和性调度 可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。
  #如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。
  #当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。
  #Affinity主要分为三类：
  #nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题
  #podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题
  #podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题
  # 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      # 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可
        nodeSelectorTerms:
        # 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功
          - matchExpressions:
              - key: nodeenv
                operator: In
                values: ["xxx", "yyy"]
              - key: nodeenv # 匹配存在标签的key为nodeenv的节点
                operator: Exists
              - key: nodecpuCount # 匹配标签的key为nodecpuCount,且value大于2的节点
                operator: Gt
                values: ["2"]

      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1 
          preference: 
          - matchExpressions:
            - key: nodeenv 
              operator: In
              values: ["pro"]
            

        - weight: 10
          preference: 

        - weight:  5
          preference: 
    podAffinity: 
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector: 
          - matchExpressions:
            - key: nodeenv
              operator: In
              values: ["pro"] 
          topologyKey: kubernetes.io/hostname  #如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围,如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分

      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            - labelSelector: 
              - matchExpressions:
                - key: nodeenv
                  operator: In 
                  values: ["test"]
              topologyKey: beta.kubernetes.io/os
              namespaces: ["netcore","netstandard"]

          weight: 1

        - podAffinityTerm:
            topologyKey: kubernetes.io/hostname
          weight: 10
    # PodAffinityPodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。
    podAntiAffinity: 
    #PodAntiAffinity PodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。
      requiredDuringSchedulingIgnoredDuringExecution: 
        - labelSelector:
            - matchExpressions:
              - key: nodeenv
                operator: In
                values: ["pro"]
            - matchLables: 
              - key: version
                value: 2.0.0.01
          topologyKey: kubernetes.io/hostname
          namespaces: ["netcore","netstandard"]
        
      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            - labelSelector:
              - matchExpressions:
                - key: python
                  operator: Exists
              topologyKey: beta.kubernetes.io/os
          weight: 20
          
        - podAffinityTerm:
            topologyKey: beta.kubernetes.io/os
          weight: 10


  #容忍
  tolerations: 
    - key: "tag"
      operator: "Equal"
      value: "2.0"
      effect: "NoExecute"
      tolerationSeconds:  3600*24*7 # 容忍时间 在这段时间里pod不会被evict 

# 通过在Node上添加污点属性，来决定是否允许Pod调度过来
# Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。
# 污点的格式为：key=value:effect, key和value是污点的标签，effect描述污点的作用，支持如下三个选项：

#PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度
#NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod
#NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离
# 可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到容忍。
# ，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝






































